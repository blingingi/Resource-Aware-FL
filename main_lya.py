#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Python version: 3.6

import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import copy
import numpy as np
from torchvision import datasets, transforms
import torch
import torch.nn.functional as F
import os
import datetime

# å¼•å…¥ cifar_noniid
from utils.sampling import mnist_iid, mnist_noniid, mnist_dirichlet, cifar_iid, cifar_noniid, cifar_dirichlet
from utils.options import args_parser
from models.Update import LocalUpdate
from models.Nets import MLP, CNNMnist, CNNCifar
from models.Fed import FedAvg
from models.test import test_img

# å¯¼å…¥ä½ çš„èµ„æºç®¡ç†å™¨å’Œè®¡ç®—å·¥å…·
from utils.resource import ResourceManager
from utils.sim_div import get_weight_difference, compute_cosine_similarity

if __name__ == '__main__':
    # parse args
    args = args_parser()
    args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')

    # load dataset and split users
    if args.dataset == 'mnist':
        trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])
        dataset_train = datasets.MNIST('../data/mnist/', train=True, download=True, transform=trans_mnist)
        dataset_test = datasets.MNIST('../data/mnist/', train=False, download=True, transform=trans_mnist)
        if args.partition == 'iid':
            print("=> æ­£åœ¨ä½¿ç”¨ IID å‡åŒ€åˆ’åˆ† MNIST æ•°æ®...")
            dict_users = mnist_iid(dataset_train, args.num_users)
        elif args.partition == 'shard':
            print("=> æ­£åœ¨ä½¿ç”¨ Shard åˆ†ç‰‡åˆ’åˆ† MNIST æ•°æ®...")
            dict_users = mnist_noniid(dataset_train, args.num_users)
        elif args.partition == 'dirichlet':
            print(f"=> æ­£åœ¨ä½¿ç”¨ Dirichlet åˆ’åˆ† MNIST æ•°æ®, alpha={args.alpha}...")
            dict_users = mnist_dirichlet(dataset_train, args.num_users, args.alpha)
        else:
            exit('Error: unrecognized partition strategy')
            
    elif args.dataset == 'cifar':
        trans_train = transforms.Compose([
            transforms.RandomCrop(32, padding=4),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
        ])
        trans_test = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
        ])
        dataset_train = datasets.CIFAR10('../data/cifar', train=True, download=True, transform=trans_train)
        dataset_test = datasets.CIFAR10('../data/cifar', train=False, download=True, transform=trans_test)
        
        if args.partition=='iid':
            print("=> æ­£åœ¨ä½¿ç”¨ IID å‡åŒ€åˆ’åˆ†æ•°æ®...")
            dict_users = cifar_iid(dataset_train, args.num_users)
        elif args.partition=='shard':
            print("=> æ­£åœ¨ä½¿ç”¨ Shard åˆ†ç‰‡åˆ’åˆ†æ•°æ®...")
            dict_users = cifar_noniid(dataset_train, args.num_users)
        elif args.partition == 'dirichlet':
            print(f"=> æ­£åœ¨ä½¿ç”¨ Dirichlet åˆ’åˆ†æ•°æ®, alpha={args.alpha}...")
            dict_users = cifar_dirichlet(dataset_train, args.num_users, args.alpha, args.local_bs)
        else:
            exit('Error: unrecognized partition strategy')
    else:
        exit('Error: unrecognized dataset')

    img_size = dataset_train[0][0].shape

    # build model
    if args.model == 'cnn' and args.dataset == 'cifar':
        net_glob = CNNCifar(args=args).to(args.device)
    elif args.model == 'cnn' and args.dataset == 'mnist':
        net_glob = CNNMnist(args=args).to(args.device)
    elif args.model == 'mlp':
        len_in = 1
        for x in img_size:
            len_in *= x
        net_glob = MLP(dim_in=len_in, dim_hidden=200, dim_out=args.num_classes).to(args.device)
    else:
        exit('Error: unrecognized model')
        
    print(net_glob)
    net_glob.train()

    # copy weights
    w_glob = net_glob.state_dict()

    # training
    loss_train = []
    acc_test_history = [] 

    # === [æ ¸å¿ƒé‡æ„] åˆå§‹åŒ–èµ„æºç®¡ç†å™¨ä¸æé›…æ™®è¯ºå¤«é˜Ÿåˆ— ===
    # å¿…é¡»ä¼ å…¥ dict_usersï¼Œèµ„æºç®¡ç†å™¨æ‰èƒ½æ ¹æ®çœŸå®æ•°æ®é‡é™æ€è®¡ç®—æ‰€æœ‰èŠ‚ç‚¹çš„å¼€é”€
    resource_mgr = ResourceManager(args.num_users, dict_users, limit_ratio=0.8)
    
    # æƒè¡¡å‚æ•° Vï¼šè¶Šå¤§è¶Šçœ‹é‡æ¨¡å‹å‡†ç¡®ç‡ (SIM/DIV)ï¼Œè¶Šå°è¶Šçœ‹é‡èµ„æºé™åˆ¶ã€‚
    # å»ºè®®å€¼ï¼š5.0~50.0 è§†å…·ä½“åˆ†æ•°æ•°é‡çº§è€Œå®š
    V = 5.0  

    # === åˆå§‹åŒ–å…¨å±€æ›´æ–°æ–¹å‘å‚è€ƒ ===
    flat_w_glob = get_weight_difference(w_glob, w_glob) 
    global_update_dir = torch.zeros_like(flat_w_glob).cpu()

    alpha = 0.8  # SIMçš„æƒé‡
    beta = 0.2   # DIVçš„æƒé‡

    for iter in range(args.epochs):
        
        m = max(int(args.frac * args.num_users), 1)
        pool_size = min(m * 2, args.num_users) 
        
        candidate_idxs = np.random.choice(range(args.num_users), pool_size, replace=False)
        
        candidate_updates = {}
        candidate_w_locals = {}
        candidate_losses = {}
        candidate_lens = {}

        print(f"\n--- Round {iter} ---")
        
        for idx in candidate_idxs:
            local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])
            w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))
            
            update_vec = get_weight_difference(w, w_glob).cpu()
            candidate_updates[idx] = update_vec
            
            cpu_w = {k: v.cpu() for k, v in w.items()}
            candidate_w_locals[idx] = cpu_w
            
            candidate_losses[idx] = loss
            candidate_lens[idx] = len(dict_users[idx])
            
            torch.cuda.empty_cache()

        # 3. åŸºäº Lyapunov (SIM + DIV - èµ„æºé˜Ÿåˆ—æƒ©ç½š) è´ªå¿ƒæŒ‘é€‰
        selected_idxs = []
        
        if iter == 0 or global_update_dir.norm() == 0:
            selected_idxs = np.random.choice(candidate_idxs, m, replace=False).tolist()
        else:
            sim_scores = {}
            for idx in candidate_idxs:
                sim_scores[idx] = compute_cosine_similarity(candidate_updates[idx], global_update_dir)
            
            remaining_candidates = list(candidate_idxs)
            
            for _ in range(m):
                best_score = -float('inf')
                best_idx = -1
                
                for idx in remaining_candidates:
                    # (1) å‡†ç¡®ç‡æ”¶ç›Šé¡¹ (Utility)
                    utility = alpha * sim_scores[idx]
                    div_penalty = 0.0
                    if len(selected_idxs) > 0:
                        for selected_idx in selected_idxs:
                            div_penalty += compute_cosine_similarity(
                                candidate_updates[idx], candidate_updates[selected_idx]
                            )
                    utility -= beta * div_penalty
                    
                    # (2) èµ„æºæƒ©ç½šé¡¹ (ç›´æ¥ä»ç®¡ç†å™¨è·å– Q * Cost)
                    resource_penalty = resource_mgr.get_penalty(idx)
                    
                    # (3) ç»“åˆ V å€¼çš„æœ€ç»ˆå¾—åˆ†
                    final_score = V * utility - resource_penalty
                    
                    if final_score > best_score:
                        best_score = final_score
                        best_idx = idx
                
                selected_idxs.append(best_idx)
                remaining_candidates.remove(best_idx)
                
        print(f"Selected clients: {selected_idxs}")
        
        # === [æ ¸å¿ƒé‡æ„] æé›…æ™®è¯ºå¤«é˜Ÿåˆ—æ¼”è¿› ===
        # ä»…éœ€ä¸€è¡Œä»£ç ï¼Œå†…éƒ¨è‡ªåŠ¨å¤„ç†æ‰€æœ‰ 100 ä¸ªå®¢æˆ·ç«¯çš„é˜Ÿåˆ—æƒ©ç½šä¸æ¢å¤
        avg_q_t, avg_q_e = resource_mgr.update_queues_and_counts(selected_idxs)

        # 4. æå–è¢«é€‰ä¸­èŠ‚ç‚¹çš„æ•°æ®å¹¶èšåˆ
        w_locals = [candidate_w_locals[idx] for idx in selected_idxs]
        len_locals = [candidate_lens[idx] for idx in selected_idxs]
        loss_locals = [candidate_losses[idx] for idx in selected_idxs]

        w_glob_new = FedAvg(w_locals, len_locals)
        
        current_global_update = get_weight_difference(w_glob_new, w_glob)
        global_update_dir = 0.9 * global_update_dir + 0.1 * current_global_update

        w_glob = w_glob_new
        net_glob.load_state_dict(w_glob)

        loss_avg = sum(loss_locals) / len(loss_locals)
        loss_train.append(loss_avg)

        net_glob.eval() 
        acc_test, loss_test = test_img(net_glob, dataset_test, args)
        acc_test_history.append(acc_test)
        
        # æ‰“å°å½“å‰è½®æ¬¡çš„å¹³å‡é˜Ÿåˆ—é•¿åº¦ï¼Œç”¨äºç›‘æ§ç³»ç»Ÿèµ„æºçŠ¶æ€
        print('Round {:3d}, Loss {:.3f}, Acc {:.2f}%, Avg Q_E: {:.3f}, Avg Q_T: {:.3f}'.format(
            iter, loss_avg, acc_test, avg_q_e, avg_q_t))
        net_glob.train()
        # å…¨å±€å­¦ä¹ ç‡è¡°å‡ï¼šæ¯è½®ä¹˜ 0.99
        # è·‘åˆ° 200 è½®æ—¶ï¼Œå­¦ä¹ ç‡å¤§çº¦ä¼šå¹³æ»‘è¡°å‡åˆ°åˆå§‹å€¼çš„ 13%ï¼Œè¿™æ˜¯ååˆ†ç»å…¸çš„è®¾å®šã€‚
        args.lr = args.lr * 0.99

    # ================= [ç»˜å›¾ä¸ä¿å­˜ç»“æœ] =================
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    script_name = os.path.basename(__file__).split('.')[0]
    
    file_id = 'fed_{}_{}_{}_alpha{}_ep{}_V{}_{}'.format(
        script_name, args.dataset, args.partition, args.alpha, args.epochs, V, timestamp)

    # å¢åŠ é˜²å´©æºƒç›®å½•æ£€æŸ¥ï¼Œç¡®ä¿ save æ–‡ä»¶å¤¹å­˜åœ¨
    save_dir = './save'
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)

    save_path = os.path.join(save_dir, '{}_acc.npy'.format(file_id))
    np.save(save_path, acc_test_history)
    
    print(f"ğŸ‰ å®éªŒç»“æŸï¼æ•°æ®å·²ç»å¯¹å®‰å…¨åœ°ä¿å­˜åˆ°: {save_path}")