#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Python version: 3.6

import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import copy
import numpy as np
import torch
import os
from torchvision import datasets, transforms
from torch import nn

# å¼•å…¥é¡¹ç›®ä¾èµ–
from utils.sampling import mnist_iid, mnist_noniid, mnist_dirichlet, cifar_iid, cifar_noniid, cifar_dirichlet
from utils.options import args_parser
from models.Update import LocalUpdate
# ç¡®ä¿ Nets é‡Œçš„ CNNCifar å·²ç»æ˜¯ä½ ä¿®æ”¹è¿‡çš„ 3å±‚å®½ä½“ç‰ˆæœ¬
from models.Nets import MLP, CNNMnist, CNNCifar
from models.Fed import FedAvg
from models.test import test_img
from utils.resource import ResourceManager
from utils.sim_div import calculate_diversity, calculate_similarity_score

if __name__ == '__main__':
    args = args_parser()
    args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')

    # ================= [Load Dataset] =================
    if args.dataset == 'mnist':
        trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])
        dataset_train = datasets.MNIST('../data/mnist/', train=True, download=True, transform=trans_mnist)
        dataset_test = datasets.MNIST('../data/mnist/', train=False, download=True, transform=trans_mnist)
        
        # å…¼å®¹ä¸åŒçš„åˆ’åˆ†æ–¹å¼
        if args.partition == 'iid':
            dict_users = mnist_iid(dataset_train, args.num_users)
        elif args.partition == 'shard':
            dict_users = mnist_noniid(dataset_train, args.num_users)
        elif args.partition == 'dirichlet':
            try:
                from utils.sampling import mnist_dirichlet
                dict_users = mnist_dirichlet(dataset_train, args.num_users, args.alpha, args.local_bs)
            except ImportError:
                dict_users = cifar_dirichlet(dataset_train, args.num_users, args.alpha, args.local_bs)
        else:
            exit('Error: unrecognized partition strategy for MNIST')
            
    elif args.dataset == 'cifar':
        trans_train = transforms.Compose([
            transforms.RandomCrop(32, padding=4),     # éšæœºè£å‰ªï¼ˆæ ‡å‡†CIFARå¢å¼ºï¼‰
            transforms.RandomHorizontalFlip(),        # éšæœºæ°´å¹³ç¿»è½¬
            transforms.ToTensor(),
            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
        ])
        trans_test = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
        ])
        dataset_train = datasets.CIFAR10('../data/cifar', train=True, download=True, transform=trans_train)
        dataset_test = datasets.CIFAR10('../data/cifar', train=False, download=True, transform=trans_test)
        
        if args.partition == 'iid':
            dict_users = cifar_iid(dataset_train, args.num_users)
        elif args.partition == 'shard':
            dict_users = cifar_noniid(dataset_train, args.num_users)
        elif args.partition == 'dirichlet':
            dict_users = cifar_dirichlet(dataset_train, args.num_users, args.alpha, args.local_bs)
        else:
            exit('Error: unrecognized partition strategy for CIFAR')
    else:
        exit('Error: unrecognized dataset')

    # ================= [Build Model] =================
    if args.dataset == 'mnist':
        net_glob = CNNMnist(args=args).to(args.device)
    elif args.dataset == 'cifar':
        net_glob = CNNCifar(args=args).to(args.device)
    
    print(net_glob)
    net_glob.train()
    w_glob = net_glob.state_dict()

    # ================= [ç­–ç•¥åˆå§‹åŒ–] =================
    print("æ­£åœ¨è®¡ç®—æ‰€æœ‰å®¢æˆ·ç«¯çš„æ•°æ®å¤šæ ·æ€§ (Diversity)...")
    div_scores = calculate_diversity(dataset_train, dict_users, args.num_classes)
    div_min, div_max = div_scores.min(), div_scores.max()
    div_norm = (div_scores - div_min) / (div_max - div_min + 1e-8)
    
    sim_scores = np.ones(args.num_users) * 10.0 # åˆå§‹ç»™ä¸ªé«˜åˆ†
    
    resource_mgr = ResourceManager(args.num_users)
    loss_train = []
    acc_test_history = [] 

    if args.all_clients: 
        print("Aggregation over all clients")
        w_locals = [w_glob for i in range(args.num_users)]

    # ================= [è”é‚¦å­¦ä¹ ä¸»å¾ªç¯] =================
    for iter in range(args.epochs):
        loss_locals = []
        len_locals = [] 
        
        if not args.all_clients:
            w_locals = []
            
        m = max(int(args.frac * args.num_users), 1)

        # ================= [Selection Strategy (OURS - è´ªå¿ƒèƒŒåŒ… + å¼¹æ€§é™çº§)] =================
        sim_min, sim_max = sim_scores.min(), sim_scores.max()
        sim_norm = (sim_scores - sim_min) / (sim_max - sim_min + 1e-8)
        
        # ä½¿ç”¨ç¨³å¥çš„æƒé‡æ¯”ä¾‹ï¼šå¤šæ ·æ€§ 0.5ï¼Œç›¸ä¼¼æ€§ 0.5 (åœ¨ Dir=0.1 ä¸‹é˜²æ­¢æç«¯åå‘)
        alpha_div_dynamic = 0.5  
        alpha_sim_dynamic = 0.5
        data_utility_scores = alpha_sim_dynamic * sim_norm + alpha_div_dynamic * (1 - div_norm)
        
        if not hasattr(resource_mgr, 'wait_times'):
            resource_mgr.wait_times = np.zeros(args.num_users)
            
        valid_candidates = []
        roi_scores_dict = {}
        client_workload_ratio = {} # ã€æ–°å¢ã€‘ï¼šè®°å½•æ¯ä¸ªå®¢æˆ·ç«¯æœ¬è½®è¢«å…è®¸å®Œæˆçš„å·¥ä½œé‡æ¯”ä¾‹
        
        # 1. è®¡ç®—å¼¹æ€§æ¯”ä¾‹ä¸æ€§ä»·æ¯” (ROI)
        for i in range(args.num_users):
            t, e = resource_mgr.calculate_cost(i, len(dict_users[i]))
            
            # ã€æ ¸å¿ƒçªç ´ï¼šå¼¹æ€§é™çº§ Partial Workã€‘
            if t > args.max_time:
                # è¶…æ—¶äº†ï¼Ÿä¸è¸¢äººï¼æŒ‰æ¯”ä¾‹ç¼©å‡å·¥ä½œé‡ã€è€—æ—¶å’Œèƒ½è€—
                ratio = args.max_time / t
                t_adjusted = args.max_time
                e_adjusted = e * ratio
            else:
                ratio = 1.0
                t_adjusted = t
                e_adjusted = e
                
            client_workload_ratio[i] = ratio
            valid_candidates.append(i) # æ‰€æœ‰äººéƒ½èƒ½è¿›å€™é€‰æ± ï¼Œä¿ç•™æ‰€æœ‰ç‰¹å¾ï¼
            
            # çº¿æ€§æ¸©å’Œè¡¥å¿ï¼ˆé˜²é¥¿æ­»æœºåˆ¶ï¼‰ï¼Œç»å¯¹å°é¡¶ 2.5 å€
            if resource_mgr.wait_times[i] > 10:
                raw_bonus = 1.0 + 0.1 * (resource_mgr.wait_times[i] - 10)
                wait_bonus = min(raw_bonus, 2.5) 
            else:
                wait_bonus = 1.0
            
            # è®¡ç®—ç»ˆææ€§ä»·æ¯” (ROI)ï¼šç”±äºè®­ç»ƒæ•°æ®æ‰“äº†æŠ˜æ‰£ï¼ŒUtilityä¹Ÿè¦æ‰“æŠ˜æ‰£ï¼
            roi = ((data_utility_scores[i] * ratio) / (e_adjusted + 1e-5)) * wait_bonus
            roi_scores_dict[i] = roi

        # 2. è´ªå¿ƒæ’åºï¼šæŒ‰æ€§ä»·æ¯” (ROI) ä»é«˜åˆ°ä½é™åºæ’åˆ—
        valid_candidates.sort(key=lambda x: roi_scores_dict[x], reverse=True)
        
        # 3. èƒ½è€—çº¦æŸèƒŒåŒ…ï¼šä»æœ€é«˜æ€§ä»·æ¯”å¼€å§‹è£…ç®±
        selected_users = []
        current_energy_sum = 0.0
        current_max_time = 0.0
        
        for client_id in valid_candidates:
            if len(selected_users) >= m:
                break # å·²ç»é€‰æ»¡äº† m ä¸ªäºº
                
            # æ‹¿åˆ°æ‰“æŠ˜åçš„çœŸå®èƒ½è€—å’Œæ—¶å»¶
            _, original_e = resource_mgr.calculate_cost(client_id, len(dict_users[client_id]))
            ratio = client_workload_ratio[client_id]
            e_adjusted = original_e * ratio
            
            # æ£€æŸ¥æ‰“æŠ˜åçš„èƒ½è€—é¢„ç®—
            if current_energy_sum + e_adjusted <= args.max_energy:
                selected_users.append(client_id)
                current_energy_sum += e_adjusted
                
        # å…œåº•æœºåˆ¶ï¼šå¦‚æœèƒ½è€—æåº¦ç´§å¼ ï¼Œå¯¼è‡´è¿ä¸€ä¸ªäººéƒ½è£…ä¸ä¸‹
        if len(selected_users) == 0:
             print("âš ï¸ èƒ½è€—é¢„ç®—æå…¶ä¸¥è‹›ï¼Œè§¦å‘å…œåº•ï¼å¯»æ‰¾æ‰“æŠ˜åèƒ½è€—æœ€å°çš„å®¢æˆ·ç«¯ã€‚")
             best_fallback = min(valid_candidates, key=lambda x: resource_mgr.calculate_cost(x, len(dict_users[x]))[1] * client_workload_ratio[x])
             selected_users.append(best_fallback)
             current_energy_sum += resource_mgr.calculate_cost(best_fallback, len(dict_users[best_fallback]))[1] * client_workload_ratio[best_fallback]

        # ç»Ÿè®¡æœ¬è½®çœŸå®æœ€å¤§æ—¶å»¶
        for su in selected_users:
            t, _ = resource_mgr.calculate_cost(su, len(dict_users[su]))
            t_adj = t * client_workload_ratio[su] if t > args.max_time else t
            current_max_time = max(current_max_time, t_adj)

        # 4. æ›´æ–°ç­‰å¾…é™ˆæ—§åº¦
        resource_mgr.wait_times += 1 
        for su in selected_users:
            resource_mgr.wait_times[su] = 0 
            
        resource_mgr.update_selection(selected_users)
        
        print(f"Round {iter:3d} | é€‰ä¸­ {len(selected_users):2d} äºº | "
              f"æ—¶å»¶: {current_max_time:.2f}s | è€—èƒ½: {current_energy_sum:.2f}J")
        
        # ================= [Local Training] =================
        for idx in selected_users:
            ratio = client_workload_ratio[idx]
            # ã€æˆªæ–­æ•°æ®ã€‘ï¼šæ ¹æ® ratio å†³å®šç»™è¿™ä¸ªå®¢æˆ·ç«¯çœŸå®æ´¾å‘å¤šå°‘æ•°æ®
            actual_sample_num = max(int(len(dict_users[idx]) * ratio), 1)
            actual_idxs = dict_users[idx][:actual_sample_num] # åªå–å‰ N ä¸ªæ ·æœ¬
            
            # è¿™é‡Œä¼ è¿›å»çš„æ˜¯æˆªæ–­åçš„æ•°æ®ç´¢å¼• actual_idxsï¼
            local = LocalUpdate(args=args, dataset=dataset_train, idxs=actual_idxs)
            w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))
            
            new_sim = calculate_similarity_score(w_glob, w)
            sim_scores[idx] = new_sim
            
            if args.all_clients:
                w_locals[idx] = copy.deepcopy(w)
            else:
                w_locals.append(copy.deepcopy(w))
            loss_locals.append(copy.deepcopy(loss))
            
            # ã€æå…¶é‡è¦ã€‘ï¼šèšåˆæƒé‡å¿…é¡»æŒ‰ç…§å®ƒâ€œå®é™…è®­ç»ƒçš„æ ·æœ¬æ•°â€æ¥ç®—ï¼
            len_locals.append(actual_sample_num)
            
        # å°†æƒé‡ä¼ é€’ç»™åŠ æƒèšåˆå‡½æ•°
        w_glob = FedAvg(w_locals, len_locals)
        net_glob.load_state_dict(w_glob)

        loss_avg = sum(loss_locals) / len(loss_locals)
        loss_train.append(loss_avg)

        # ================= [Evaluation] =================
        net_glob.eval()
        acc_test, loss_test = test_img(net_glob, dataset_test, args)
        acc_test_history.append(acc_test)
        print('  ==> Average loss {:.3f}, Test Acc {:.2f}%'.format(loss_avg, acc_test))
        net_glob.train()
        args.lr = args.lr * 0.99

    # ================= [ç»˜å›¾ä¸ä¿å­˜ç»“æœ] =================
    import datetime
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    script_name = os.path.basename(__file__).split('.')[0]
    
    file_id = 'fed_{}_{}_{}_alpha{}_ep{}_time{}_energy{}_{}'.format(
        script_name, 
        args.dataset, 
        args.partition, 
        args.alpha, 
        args.epochs, 
        args.max_time,   
        args.max_energy, 
        timestamp
    )

    save_dir = './save'
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)

    save_path = os.path.join(save_dir, '{}_acc.npy'.format(file_id))
    np.save(save_path, acc_test_history)
    
    print(f"ğŸ‰ å®éªŒç»“æŸï¼æ•°æ®å·²ç»å¯¹å®‰å…¨åœ°ä¿å­˜åˆ°: {save_path}")