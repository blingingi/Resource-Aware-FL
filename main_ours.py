#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Python version: 3.6

import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import copy
import numpy as np
import torch
import os
from torchvision import datasets, transforms
from torch import nn

# å¼•å…¥é¡¹ç›®ä¾èµ–
from utils.sampling import mnist_iid, mnist_noniid, mnist_dirichlet,cifar_iid, cifar_noniid, cifar_dirichlet
from utils.options import args_parser
from models.Update import LocalUpdate
# ç¡®ä¿ Nets é‡Œçš„ CNNCifar å·²ç»æ˜¯ä½ ä¿®æ”¹è¿‡çš„ 3å±‚å®½ä½“ç‰ˆæœ¬
from models.Nets import MLP, CNNMnist, CNNCifar
from models.Fed import FedAvg
from models.test import test_img
from utils.resource import ResourceManager

# ===================================================================
# è¾…åŠ©å‡½æ•°ï¼šè®¡ç®— KL æ•£åº¦ (Diversity Metric)
# ===================================================================
def calculate_diversity(dataset, dict_users, num_classes):
    diversity_scores = []
    P_uniform = np.ones(num_classes) / num_classes 
    
    print("æ­£åœ¨è®¡ç®—æ‰€æœ‰å®¢æˆ·ç«¯çš„æ•°æ®å¤šæ ·æ€§ (Diversity)...")
    
    for idx in range(len(dict_users)):
        user_indices = dict_users[idx]
        if hasattr(dataset, 'targets'):
            labels = np.array(dataset.targets)[list(user_indices)]
        else:
            labels = dataset.train_labels.numpy()[list(user_indices)]
            
        label_counts = np.zeros(num_classes)
        for label in labels:
            label_counts[label] += 1
        
        P_client = (label_counts + 1e-5) / (sum(label_counts) + 1e-5 * num_classes)
        kl_div = np.sum(P_uniform * np.log(P_uniform / P_client))
        diversity_scores.append(kl_div)
        
    return np.array(diversity_scores)

# ===================================================================
# è¾…åŠ©å‡½æ•°ï¼šè®¡ç®—ç›¸ä¼¼æ€§ (Similarity Metric)
# åªè®¡ç®—æœ€åä¸€å±‚ (fc2) çš„å‚æ•°è·ç¦»ï¼Œé˜²æ­¢è·ç¦»è¿‡å¤§å¯¼è‡´åˆ†æ•°å½’é›¶
# ===================================================================
def calculate_similarity_score(w_global, w_local, k1=10, k2=0.01):
    diff_sum = 0
    target_layer = 'fc2' 
    
    layer_found = False
    for k in w_global.keys():
        if target_layer in k:
            diff_sum += torch.sum(torch.abs(w_global[k] - w_local[k])).item()
            layer_found = True
            
    if not layer_found:
        total_diff = 0
        total_params = 0
        for k in w_global.keys():
            total_diff += torch.sum(torch.abs(w_global[k] - w_local[k])).item()
            total_params += w_global[k].numel()
        diff_sum = total_diff
    
    rho = diff_sum
    sim = k1 * np.exp(-k2 * rho)
    return sim

if __name__ == '__main__':
    args = args_parser()
    args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')

# ================= [Load Dataset] =================
    if args.dataset == 'mnist':
        trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])
        dataset_train = datasets.MNIST('../data/mnist/', train=True, download=True, transform=trans_mnist)
        dataset_test = datasets.MNIST('../data/mnist/', train=False, download=True, transform=trans_mnist)
        
        # å…¼å®¹ä¸åŒçš„åˆ’åˆ†æ–¹å¼
        if args.partition == 'iid':
            dict_users = mnist_iid(dataset_train, args.num_users)
        elif args.partition == 'shard':
            dict_users = mnist_noniid(dataset_train, args.num_users)
        elif args.partition == 'dirichlet':
            # å¦‚æœä½ çš„ sampling.py é‡Œæœ‰ mnist_dirichlet å°±ç”¨å®ƒï¼Œæ²¡æœ‰çš„è¯ç›´æ¥ç”¨ cifar_dirichlet å¤„ç† MNIST æ ‡ç­¾ä¹Ÿæ˜¯ä¸€æ ·çš„
            try:
                from utils.sampling import mnist_dirichlet
                dict_users = mnist_dirichlet(dataset_train, args.num_users, args.alpha)
            except ImportError:
                dict_users = cifar_dirichlet(dataset_train, args.num_users, args.alpha)
        else:
            exit('Error: unrecognized partition strategy for MNIST')
            
    elif args.dataset == 'cifar':
        trans_cifar = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
        dataset_train = datasets.CIFAR10('../data/cifar', train=True, download=True, transform=trans_cifar)
        dataset_test = datasets.CIFAR10('../data/cifar', train=False, download=True, transform=trans_cifar)
        
        if args.partition == 'iid':
            dict_users = cifar_iid(dataset_train, args.num_users)
        elif args.partition == 'shard':
            dict_users = cifar_noniid(dataset_train, args.num_users)
        elif args.partition == 'dirichlet':
            dict_users = cifar_dirichlet(dataset_train, args.num_users, args.alpha)
        else:
            exit('Error: unrecognized partition strategy for CIFAR')
    else:
        exit('Error: unrecognized dataset')

    # ================= [Build Model] =================
    if args.dataset == 'mnist':
        net_glob = CNNMnist(args=args).to(args.device)
    elif args.dataset == 'cifar':
        net_glob = CNNCifar(args=args).to(args.device)
    
    print(net_glob)
    net_glob.train()
    w_glob = net_glob.state_dict()

    # ================= [ç­–ç•¥ä¿®æ­£] =================
    # 1. Diversity
    div_scores = calculate_diversity(dataset_train, dict_users, args.num_classes)
    div_min, div_max = div_scores.min(), div_scores.max()
    div_norm = (div_scores - div_min) / (div_max - div_min + 1e-8)
    
    # 2. Similarity
    sim_scores = np.ones(args.num_users) * 10.0 # åˆå§‹ç»™ä¸ªé«˜åˆ†
    
    # 3. æƒé‡å‚æ•°
    alpha_1 = 0.2  # Similarity æƒé‡
    alpha_2 = 0.8  # Diversity æƒé‡
    # ============================================
    resource_mgr = ResourceManager(args.num_users)
    loss_train = []
    acc_test_history = [] 

    if args.all_clients: 
        print("Aggregation over all clients")
        w_locals = [w_glob for i in range(args.num_users)]

    for iter in range(args.epochs):
        loss_locals = []
        len_locals = [] # ã€ä¿®å¤1ã€‘åˆå§‹åŒ–å½“å‰è½®æ¬¡çš„æ•°æ®é‡è®°å½•åˆ—è¡¨
        
        if not args.all_clients:
            w_locals = []
            
        m = max(int(args.frac * args.num_users), 1)

       # ================= [Selection Strategy (OURS v4 - Probabilistic ROI)] =================
        sim_min, sim_max = sim_scores.min(), sim_scores.max()
        sim_norm = (sim_scores - sim_min) / (sim_max - sim_min + 1e-8)
        
        # 1. è‡ªé€‚åº”åŠ¨æ€æƒé‡åˆ†é…
        progress = iter / args.epochs
        alpha_div_dynamic = np.exp(-3.0 * progress) 
        alpha_sim_dynamic = 1.0 - alpha_div_dynamic 
        data_utility_scores = alpha_sim_dynamic * sim_norm + alpha_div_dynamic * (1 - div_norm)
        
        if not hasattr(resource_mgr, 'wait_times'):
            resource_mgr.wait_times = np.zeros(args.num_users)
            
        # 2. èµ„æºçº¢çº¿åˆç­›ä¸ ROI æ¦‚ç‡åŒ–
        valid_candidates = []
        roi_scores = []
        
        for i in range(args.num_users):
            t, e = resource_mgr.calculate_cost(i, len(dict_users[i]))
            # åªæœ‰å•ä¸ªèŠ‚ç‚¹è¾¾æ ‡ï¼Œæ‰æœ‰èµ„æ ¼è¿›å…¥å€™é€‰æ± 
            if t <= args.max_time and e <= args.max_energy:
                valid_candidates.append(i)
                wait_bonus = 1.0 + 0.1 * resource_mgr.wait_times[i]
                # è®¡ç®—ç»¼åˆæ€§ä»·æ¯”å¾—åˆ†
                roi = (data_utility_scores[i] / (e + 1e-5)) * wait_bonus
                roi_scores.append(roi)
                
        # å…œåº•æœºåˆ¶
        if len(valid_candidates) == 0:
            print("âš ï¸ çº¦æŸè¿‡ä¸¥ï¼Œè§¦å‘å…œåº•ï¼")
            times = [resource_mgr.calculate_cost(i, len(dict_users[i]))[0] for i in range(args.num_users)]
            valid_candidates = [np.argmin(times)]
            roi_scores = [1.0]

        # 3. ã€æ ¸å¿ƒä¿®æ­£ã€‘ï¼šæ¦‚ç‡è½®ç›˜èµŒï¼Œæ¢å¤è”é‚¦å­¦ä¹ çš„éšæœºæ€§ï¼
        roi_scores = np.array(roi_scores)
        p_values = roi_scores / np.sum(roi_scores)
        p_values = p_values.astype('float64')
        p_values = p_values / np.sum(p_values) # å¼ºåˆ¶è§„é¿æµ®ç‚¹ç²¾åº¦é—®é¢˜
        
        num_to_select = min(m, len(valid_candidates))
        # æ ¹æ® ROI æ¦‚ç‡è¿›è¡ŒåŠ æƒéšæœºæŠ½æ ·ï¼Œå–ä»£æ­»æ¿çš„è´ªå¿ƒæ’åº
        priority_queue = np.random.choice(valid_candidates, num_to_select, replace=False, p=p_values)
        
        # 4. æœ€ç»ˆçš„èƒ½è€—è£…ç®±æ£€æŸ¥
        selected_users = []
        current_energy_sum = 0.0
        current_max_time = 0.0
        
        for client_id in priority_queue:
            t, e = resource_mgr.calculate_cost(client_id, len(dict_users[client_id]))
            if current_energy_sum + e <= args.max_energy:
                selected_users.append(client_id)
                current_energy_sum += e
                current_max_time = max(current_max_time, t)
                
        # 5. æ›´æ–°é™ˆæ—§åº¦
        resource_mgr.wait_times += 1 
        for su in selected_users:
            resource_mgr.wait_times[su] = 0 
            
        resource_mgr.update_selection(selected_users)
        print(f"Round {iter} | é€‰ä¸­ {len(selected_users)} äºº | "
              f"Divæƒé‡: {alpha_div_dynamic:.2f}, Simæƒé‡: {alpha_sim_dynamic:.2f} | "
              f"æ—¶å»¶: {current_max_time:.2f}s | è€—èƒ½: {current_energy_sum:.2f}J")
        # ========================================================================

        # ã€ä¿®å¤å®Œæˆã€‘åªä¿ç•™ä¸€ä¸ªå¾ªç¯ï¼Œä¸”ä¸¥æ ¼éå† selected_users
        for idx in selected_users:
            local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])
            w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))
            
            new_sim = calculate_similarity_score(w_glob, w)
            sim_scores[idx] = new_sim
            
            if args.all_clients:
                w_locals[idx] = copy.deepcopy(w)
            else:
                w_locals.append(copy.deepcopy(w))
            loss_locals.append(copy.deepcopy(loss))
            
            # ä¸¥æ ¼è®°å½•è¢«é€‰å®¢æˆ·ç«¯çš„çœŸå®æ ·æœ¬æ•°
            len_locals.append(len(dict_users[idx]))
            
        # å°†æƒé‡ä¼ é€’ç»™åŠ æƒèšåˆå‡½æ•°
        w_glob = FedAvg(w_locals, len_locals)
        net_glob.load_state_dict(w_glob)

        loss_avg = sum(loss_locals) / len(loss_locals)
        loss_train.append(loss_avg)

        # Evaluation
        net_glob.eval()
        acc_test, loss_test = test_img(net_glob, dataset_test, args)
        acc_test_history.append(acc_test)
        print('Round {:3d}, Average loss {:.3f}, Test Acc {:.2f}%'.format(iter, loss_avg, acc_test))
        net_glob.train()
        args.lr = args.lr * 0.99


    # ================= [ç»˜å›¾ä¸ä¿å­˜ç»“æœ] =================
    import datetime

    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    script_name = os.path.basename(__file__).split('.')[0]
    
    file_id = 'fed_{}_{}_{}_alpha{}_ep{}_time{}_energy{}_{}'.format(
        script_name, 
        args.dataset, 
        args.partition, 
        args.alpha, 
        args.epochs, 
        args.max_time,   # è®°å½•æ—¶é—´çº¢çº¿
        args.max_energy, # è®°å½•èƒ½è€—çº¢çº¿
        timestamp
    )

    # ã€ä¿®å¤5ã€‘å¢åŠ é˜²å´©æºƒç›®å½•æ£€æŸ¥
    save_dir = './save'
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)

    save_path = os.path.join(save_dir, '{}_acc.npy'.format(file_id))
    np.save(save_path, acc_test_history)
    
    print(f"ğŸ‰ å®éªŒç»“æŸï¼æ•°æ®å·²ç»å¯¹å®‰å…¨åœ°ä¿å­˜åˆ°: {save_path}")
