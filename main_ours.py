#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Python version: 3.6

import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import copy
import numpy as np
import torch
import os
from torchvision import datasets, transforms
from torch import nn

# å¼•å…¥é¡¹ç›®ä¾èµ–
from utils.sampling import mnist_iid, mnist_noniid, mnist_dirichlet,cifar_iid, cifar_noniid, cifar_dirichlet
from utils.options import args_parser
from models.Update import LocalUpdate
# ç¡®ä¿ Nets é‡Œçš„ CNNCifar å·²ç»æ˜¯ä½ ä¿®æ”¹è¿‡çš„ 3å±‚å®½ä½“ç‰ˆæœ¬
from models.Nets import MLP, CNNMnist, CNNCifar
from models.Fed import FedAvg
from models.test import test_img
from utils.resource import ResourceManager
from utils.sim_div import calculate_diversity, calculate_similarity_score

# ===================================================================
# è¾…åŠ©å‡½æ•°ï¼šè®¡ç®— KL æ•£åº¦ (Diversity Metric)
# ===================================================================

if __name__ == '__main__':
    args = args_parser()
    args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')

# ================= [Load Dataset] =================
    if args.dataset == 'mnist':
        trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])
        dataset_train = datasets.MNIST('../data/mnist/', train=True, download=True, transform=trans_mnist)
        dataset_test = datasets.MNIST('../data/mnist/', train=False, download=True, transform=trans_mnist)
        
        # å…¼å®¹ä¸åŒçš„åˆ’åˆ†æ–¹å¼
        if args.partition == 'iid':
            dict_users = mnist_iid(dataset_train, args.num_users)
        elif args.partition == 'shard':
            dict_users = mnist_noniid(dataset_train, args.num_users)
        elif args.partition == 'dirichlet':
            # å¦‚æœä½ çš„ sampling.py é‡Œæœ‰ mnist_dirichlet å°±ç”¨å®ƒï¼Œæ²¡æœ‰çš„è¯ç›´æ¥ç”¨ cifar_dirichlet å¤„ç† MNIST æ ‡ç­¾ä¹Ÿæ˜¯ä¸€æ ·çš„
            try:
                from utils.sampling import mnist_dirichlet
                dict_users = mnist_dirichlet(dataset_train, args.num_users, args.alpha)
            except ImportError:
                dict_users = cifar_dirichlet(dataset_train, args.num_users, args.alpha,args.local_bs)
        else:
            exit('Error: unrecognized partition strategy for MNIST')
            
    elif args.dataset == 'cifar':
        trans_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),     # éšæœºè£å‰ªï¼ˆæ ‡å‡†CIFARå¢å¼ºï¼‰
        transforms.RandomHorizontalFlip(),        # éšæœºæ°´å¹³ç¿»è½¬
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), 
                             (0.5, 0.5, 0.5))
        ])
        trans_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), 
                             (0.5, 0.5, 0.5))
        ])
        dataset_train = datasets.CIFAR10(
        '../data/cifar', 
        train=True, 
        download=True, 
        transform=trans_train
        )
        dataset_test = datasets.CIFAR10(
        '../data/cifar', 
        train=False, 
        download=True, 
        transform=trans_test
        )
        
        if args.partition == 'iid':
            dict_users = cifar_iid(dataset_train, args.num_users)
        elif args.partition == 'shard':
            dict_users = cifar_noniid(dataset_train, args.num_users)
        elif args.partition == 'dirichlet':
            dict_users = cifar_dirichlet(dataset_train, args.num_users, args.alpha,args.local_bs)
        else:
            exit('Error: unrecognized partition strategy for CIFAR')
    else:
        exit('Error: unrecognized dataset')

    # ================= [Build Model] =================
    if args.dataset == 'mnist':
        net_glob = CNNMnist(args=args).to(args.device)
    elif args.dataset == 'cifar':
        net_glob = CNNCifar(args=args).to(args.device)
    
    print(net_glob)
    net_glob.train()
    w_glob = net_glob.state_dict()

    # ================= [ç­–ç•¥ä¿®æ­£] =================
    # 1. Diversity
    div_scores = calculate_diversity(dataset_train, dict_users, args.num_classes)
    div_min, div_max = div_scores.min(), div_scores.max()
    div_norm = (div_scores - div_min) / (div_max - div_min + 1e-8)
    
    # 2. Similarity
    sim_scores = np.ones(args.num_users) * 10.0 # åˆå§‹ç»™ä¸ªé«˜åˆ†
    
    # 3. æƒé‡å‚æ•°
    alpha_1 = 0.2  # Similarity æƒé‡
    alpha_2 = 0.8  # Diversity æƒé‡
    # ============================================
    resource_mgr = ResourceManager(args.num_users)
    loss_train = []
    acc_test_history = [] 

    if args.all_clients: 
        print("Aggregation over all clients")
        w_locals = [w_glob for i in range(args.num_users)]

    for iter in range(args.epochs):
        loss_locals = []
        len_locals = [] # ã€ä¿®å¤1ã€‘åˆå§‹åŒ–å½“å‰è½®æ¬¡çš„æ•°æ®é‡è®°å½•åˆ—è¡¨
        
        if not args.all_clients:
            w_locals = []
            
        m = max(int(args.frac * args.num_users), 1)

     # ================= [Selection Strategy (OURS - è´ªå¿ƒèƒŒåŒ…è°ƒåº¦ç®—æ³•)] =================
        sim_min, sim_max = sim_scores.min(), sim_scores.max()
        sim_norm = (sim_scores - sim_min) / (sim_max - sim_min + 1e-8)
        
        # ä½¿ç”¨ç¨³å¥çš„æƒé‡æ¯”ä¾‹ï¼šå¤šæ ·æ€§ 0.5ï¼Œç›¸ä¼¼æ€§ 0.5 (é˜²æ­¢æç«¯åˆ†å¸ƒæ‹‰å´©æ¨¡å‹)
        alpha_div_dynamic = 0.5  
        alpha_sim_dynamic = 0.5
        data_utility_scores = alpha_sim_dynamic * sim_norm + alpha_div_dynamic * (1 - div_norm)
        
        if not hasattr(resource_mgr, 'wait_times'):
            resource_mgr.wait_times = np.zeros(args.num_users)
            
        # 1. æ—¶å»¶ç¡¬çº¦æŸåˆç­›ï¼šåªä¿ç•™èƒ½å¤Ÿåœ¨ max_time å†…å®Œæˆçš„å®¢æˆ·ç«¯
        valid_candidates = []
        roi_scores_dict = {}
        
        for i in range(args.num_users):
            t, e = resource_mgr.calculate_cost(i, len(dict_users[i]))
            
            if t <= args.max_time:
                valid_candidates.append(i)
                
                # çº¿æ€§æ¸©å’Œè¡¥å¿ï¼ˆé˜²é¥¿æ­»æœºåˆ¶ï¼‰ï¼Œç»å¯¹å°é¡¶ 2.5 å€
                if resource_mgr.wait_times[i] > 10:
                    raw_bonus = 1.0 + 0.1 * (resource_mgr.wait_times[i] - 10)
                    wait_bonus = min(raw_bonus, 2.5) 
                else:
                    wait_bonus = 1.0
                
                # è®¡ç®—ç»ˆææ€§ä»·æ¯” (ROI): (ä»·å€¼ / è€—èƒ½) * ç­‰å¾…è¡¥å¿
                roi = (data_utility_scores[i] / (e + 1e-5)) * wait_bonus
                roi_scores_dict[i] = roi

        # å…œåº•æœºåˆ¶ï¼šå¦‚æœæ—¶å»¶å¡å¾—å¤ªæ­»ï¼Œå¯¼è‡´å…¨å†›è¦†æ²¡
        if len(valid_candidates) == 0:
            print("âš ï¸ æ—¶å»¶çº¦æŸè¿‡ä¸¥ï¼Œè§¦å‘å…œåº•ï¼å¯»æ‰¾æœ€å¿«å®Œæˆçš„å®¢æˆ·ç«¯ã€‚")
            times = [resource_mgr.calculate_cost(i, len(dict_users[i]))[0] for i in range(args.num_users)]
            valid_candidates = [np.argmin(times)]
            roi_scores_dict[valid_candidates[0]] = 1.0

        # 2. è´ªå¿ƒæ’åºï¼šæŒ‰æ€§ä»·æ¯” (ROI) ä»é«˜åˆ°ä½é™åºæ’åˆ—
        valid_candidates.sort(key=lambda x: roi_scores_dict[x], reverse=True)
        
        # 3. èƒ½è€—çº¦æŸèƒŒåŒ…ï¼šä»æœ€é«˜æ€§ä»·æ¯”å¼€å§‹è£…ç®±ï¼Œç›´åˆ°äººæ•°æ»¡æˆ–èƒ½è€—è€—å°½
        selected_users = []
        current_energy_sum = 0.0
        current_max_time = 0.0
        
        for client_id in valid_candidates:
            if len(selected_users) >= m:
                break # å·²ç»é€‰æ»¡äº† m ä¸ªäºº
                
            _, e = resource_mgr.calculate_cost(client_id, len(dict_users[client_id]))
            
            # æ£€æŸ¥èƒ½è€—é¢„ç®—
            if current_energy_sum + e <= args.max_energy:
                selected_users.append(client_id)
                current_energy_sum += e
                
        # å…œåº•æœºåˆ¶ï¼šå¦‚æœå…¨å‘˜é«˜èƒ½è€—ï¼Œå¯¼è‡´è¿ä¸€ä¸ªäººéƒ½è£…ä¸ä¸‹
        if len(selected_users) == 0:
             print("âš ï¸ èƒ½è€—é¢„ç®—æå…¶ä¸¥è‹›ï¼Œè§¦å‘å…œåº•ï¼å¯»æ‰¾èƒ½è€—æœ€å°çš„è¾¾æ ‡å®¢æˆ·ç«¯ã€‚")
             best_fallback = min(valid_candidates, key=lambda x: resource_mgr.calculate_cost(x, len(dict_users[x]))[1])
             selected_users.append(best_fallback)
             current_energy_sum += resource_mgr.calculate_cost(best_fallback, len(dict_users[best_fallback]))[1]

        # è®¡ç®—æœ¬è½®çœŸå®æœ€å¤§æ—¶å»¶
        current_max_time = max([resource_mgr.calculate_cost(i, len(dict_users[i]))[0] for i in selected_users])

        # 4. æ›´æ–°ç­‰å¾…é™ˆæ—§åº¦
        resource_mgr.wait_times += 1 
        for su in selected_users:
            resource_mgr.wait_times[su] = 0 
            
        resource_mgr.update_selection(selected_users)
        
        print(f"Round {iter} | é€‰ä¸­ {len(selected_users)} äºº | "
              f"Div: {alpha_div_dynamic:.2f}, Sim: {alpha_sim_dynamic:.2f} | "
              f"æ—¶å»¶: {current_max_time:.2f}s | è€—èƒ½: {current_energy_sum:.2f}J")
        
        # ========================================================================

        # ã€ä¿®å¤å®Œæˆã€‘åªä¿ç•™ä¸€ä¸ªå¾ªç¯ï¼Œä¸”ä¸¥æ ¼éå† selected_users
        for idx in selected_users:
            local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])
            w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))
            
            new_sim = calculate_similarity_score(w_glob, w)
            sim_scores[idx] = new_sim
            
            if args.all_clients:
                w_locals[idx] = copy.deepcopy(w)
            else:
                w_locals.append(copy.deepcopy(w))
            loss_locals.append(copy.deepcopy(loss))
            
            # ä¸¥æ ¼è®°å½•è¢«é€‰å®¢æˆ·ç«¯çš„çœŸå®æ ·æœ¬æ•°
            len_locals.append(len(dict_users[idx]))
            
        # å°†æƒé‡ä¼ é€’ç»™åŠ æƒèšåˆå‡½æ•°
        w_glob = FedAvg(w_locals, len_locals)
        net_glob.load_state_dict(w_glob)

        loss_avg = sum(loss_locals) / len(loss_locals)
        loss_train.append(loss_avg)

        # Evaluation
        net_glob.eval()
        acc_test, loss_test = test_img(net_glob, dataset_test, args)
        acc_test_history.append(acc_test)
        print('Round {:3d}, Average loss {:.3f}, Test Acc {:.2f}%'.format(iter, loss_avg, acc_test))
        net_glob.train()
        args.lr = args.lr * 0.99


    # ================= [ç»˜å›¾ä¸ä¿å­˜ç»“æœ] =================
    import datetime

    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    script_name = os.path.basename(__file__).split('.')[0]
    
    file_id = 'fed_{}_{}_{}_alpha{}_ep{}_time{}_energy{}_{}'.format(
        script_name, 
        args.dataset, 
        args.partition, 
        args.alpha, 
        args.epochs, 
        args.max_time,   # è®°å½•æ—¶é—´çº¢çº¿
        args.max_energy, # è®°å½•èƒ½è€—çº¢çº¿
        timestamp
    )

    # ã€ä¿®å¤5ã€‘å¢åŠ é˜²å´©æºƒç›®å½•æ£€æŸ¥
    save_dir = './save'
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)

    save_path = os.path.join(save_dir, '{}_acc.npy'.format(file_id))
    np.save(save_path, acc_test_history)
    
    print(f"ğŸ‰ å®éªŒç»“æŸï¼æ•°æ®å·²ç»å¯¹å®‰å…¨åœ°ä¿å­˜åˆ°: {save_path}")
