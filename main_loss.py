#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Python version: 3.6

import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import copy
import numpy as np
from torchvision import datasets, transforms
import torch

from utils.sampling import mnist_iid, mnist_noniid,mnist_dirichlet, cifar_iid,cifar_noniid, cifar_dirichlet
from utils.options import args_parser
from models.Update import LocalUpdate
from models.Nets import MLP, CNNMnist, CNNCifar
from models.Fed import FedAvg
from models.test import test_img


if __name__ == '__main__':
    # parse args
    args = args_parser()
    args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')

    # [å¯é€‰] å›ºå®šéšæœºç§å­ (ä¸ºäº†å¤ç°å®éªŒ)
    # import random
    # if args.seed is not None:
    #     random.seed(args.seed)
    #     torch.manual_seed(args.seed)
    #     np.random.seed(args.seed)

    # load dataset and split users
    if args.dataset == 'mnist':
        trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])
        dataset_train = datasets.MNIST('../data/mnist/', train=True, download=True, transform=trans_mnist)
        dataset_test = datasets.MNIST('../data/mnist/', train=False, download=True, transform=trans_mnist)
        # sample users
        if args.iid:
            dict_users = mnist_iid(dataset_train, args.num_users)
        else:
            dict_users = mnist_noniid(dataset_train, args.num_users)
    elif args.dataset == 'cifar':
        trans_cifar = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
        dataset_train = datasets.CIFAR10('../data/cifar', train=True, download=True, transform=trans_cifar)
        dataset_test = datasets.CIFAR10('../data/cifar', train=False, download=True, transform=trans_cifar)
        if args.partition == 'iid':
            dict_users = cifar_iid(dataset_train, args.num_users)
        elif args.partition == 'shard':
            dict_users = cifar_noniid(dataset_train, args.num_users)
        elif args.partition == 'dirichlet':
            dict_users = cifar_dirichlet(dataset_train, args.num_users, args.alpha)
        else:
            exit('Error: unrecognized partition strategy')
    else:
        exit('Error: unrecognized dataset')
    img_size = dataset_train[0][0].shape

    # build model
    if args.model == 'cnn' and args.dataset == 'cifar':
        net_glob = CNNCifar(args=args).to(args.device)
    elif args.model == 'cnn' and args.dataset == 'mnist':
        net_glob = CNNMnist(args=args).to(args.device)
    elif args.model == 'mlp':
        len_in = 1
        for x in img_size:
            len_in *= x
        net_glob = MLP(dim_in=len_in, dim_hidden=200, dim_out=args.num_classes).to(args.device)
    else:
        exit('Error: unrecognized model')
    print(net_glob)
    net_glob.train()

    # copy weights
    w_glob = net_glob.state_dict()

    # training
    loss_train = []
    acc_test_history = [] 
    
    cv_loss, cv_acc = [], []
    val_loss_pre, counter = 0, 0
    net_best = None
    best_loss = None
    val_acc_list, net_list = [], []

    if args.all_clients: 
        print("Aggregation over all clients")
        w_locals = [w_glob for i in range(args.num_users)]

    # [ç­–ç•¥å‡†å¤‡] 
    # 1. åˆå§‹åŒ– Loss (ç”¨äºè®¡ç®—æ¦‚ç‡)
    client_losses = np.ones(args.num_users) * 100.0
    # 2. åˆå§‹åŒ–è®¡æ•°å™¨ (ç”¨äºç”»â€œé¢‘ç‡åˆ†å¸ƒå›¾â€)
    client_selection_count = np.zeros(args.num_users)

    for iter in range(args.epochs):
        loss_locals = []
        if not args.all_clients:
            w_locals = []
            
        # ================= [Client Selection Strategy] =================
        m = max(int(args.frac * args.num_users), 1)
        
        # åŸºäº Loss è®¡ç®—æ¦‚ç‡ (Loss è¶Šå¤§ï¼Œæ¦‚ç‡è¶Šå¤§)
        p_values = np.abs(client_losses) + 1e-8
        p_values = p_values / np.sum(p_values)
        
        try:
            idxs_users = np.random.choice(range(args.num_users), m, replace=False, p=p_values)
        except:
            # å®¹é”™å¤„ç†ï¼šå¦‚æœæ¦‚ç‡è®¡ç®—æ•°å€¼ä¸ç¨³å®šï¼Œå›é€€åˆ°éšæœº
            idxs_users = np.random.choice(range(args.num_users), m, replace=False)
            
        # [ç»Ÿè®¡] è®°å½•æœ¬è½®è°è¢«é€‰ä¸­äº†
        for i in idxs_users:
            client_selection_count[i] += 1
        # ===============================================================

        for idx in idxs_users:
            local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])
            w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))
            
            # [åé¦ˆ] è®°å½•è¯¥ç”¨æˆ·çš„ Lossï¼Œä¾›ä¸‹ä¸€è½®é€‰æ‹©ä½¿ç”¨
            client_losses[idx] = loss
            
            if args.all_clients:
                w_locals[idx] = copy.deepcopy(w)
            else:
                w_locals.append(copy.deepcopy(w))
            loss_locals.append(copy.deepcopy(loss))
            
        # update global weights
        w_glob = FedAvg(w_locals)

        # copy weight to net_glob
        net_glob.load_state_dict(w_glob)

        # print loss
        loss_avg = sum(loss_locals) / len(loss_locals)
        loss_train.append(loss_avg)

        # [è¯„ä¼°] æ¯ä¸€è½®ç»“æŸæ—¶ï¼Œè·‘ä¸€æ¬¡æµ‹è¯•å¹¶è®°å½• Accuracy
        net_glob.eval() 
        acc_test, loss_test = test_img(net_glob, dataset_test, args)
        acc_test_history.append(acc_test)
        print('Round {:3d}, Average loss {:.3f}, Test Acc {:.2f}%'.format(iter, loss_avg, acc_test))
        net_glob.train() 
        args.lr = args.lr * 0.99

 

    # ================= [ç»˜å›¾ä¸ä¿å­˜ç»“æœ] =================
    import os
    import datetime

    # 1. è·å–ç²¾ç¡®åˆ°ç§’çš„æ—¶é—´æˆ³ï¼Œç¡®ä¿ç»å¯¹ä¸è¦†ç›–
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # 2. è·å–å½“å‰è¿è¡Œè„šæœ¬çš„åå­— (è‡ªåŠ¨è¯†åˆ«æ˜¯ baseline è¿˜æ˜¯ litong_v2)
    script_name = os.path.basename(__file__).split('.')[0]
    
    # 3. ã€æ ¸å¿ƒä¿®å¤ã€‘æ–‡ä»¶åé‡ŒåŠ ä¸Š è„šæœ¬åã€alpha å’Œ timestamp
    # æ¨èä¿®æ”¹ä½ çš„ä¿å­˜å‘½åé€»è¾‘ï¼ŒåŠ å…¥ partition å­—æ®µ
    file_id = 'fed_{}_{}_{}_alpha{}_ep{}_{}'.format(
        script_name, args.dataset, args.partition, args.alpha, args.epochs, timestamp)

    # 4. ä¿å­˜åŸå§‹æ•°æ® (.npy)
    save_path = './save/{}_acc.npy'.format(file_id)
    np.save(save_path, acc_test_history)
    
    print(f"ğŸ‰ å®éªŒç»“æŸï¼æ•°æ®å·²ç»å¯¹å®‰å…¨åœ°ä¿å­˜åˆ°: {save_path}")