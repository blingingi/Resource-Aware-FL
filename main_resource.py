#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Python version: 3.6

import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import copy
import numpy as np
import torch
import os
from torchvision import datasets, transforms
from torch import nn

# å¼•å…¥é¡¹ç›®ä¾èµ–
from utils.sampling import mnist_iid, mnist_noniid, mnist_dirichlet,cifar_iid, cifar_noniid, cifar_dirichlet
from utils.options import args_parser
from models.Update import LocalUpdate
# ç¡®ä¿ Nets é‡Œçš„ CNNCifar å·²ç»æ˜¯ä½ ä¿®æ”¹è¿‡çš„ 3å±‚å®½ä½“ç‰ˆæœ¬
from models.Nets import MLP, CNNMnist, CNNCifar
from models.Fed import FedAvg
from models.test import test_img
from utils.resource import ResourceManager
from utils.sim_div import calculate_diversity, calculate_similarity_score

if __name__ == '__main__':
    args = args_parser()
    args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')

   # ================= [Load Dataset] =================
    if args.dataset == 'mnist':
        trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])
        dataset_train = datasets.MNIST('../data/mnist/', train=True, download=True, transform=trans_mnist)
        dataset_test = datasets.MNIST('../data/mnist/', train=False, download=True, transform=trans_mnist)
        
        # å…¼å®¹ä¸åŒçš„åˆ’åˆ†æ–¹å¼
        if args.partition == 'iid':
            dict_users = mnist_iid(dataset_train, args.num_users)
        elif args.partition == 'shard':
            dict_users = mnist_noniid(dataset_train, args.num_users)
        elif args.partition == 'dirichlet':
            # å¦‚æœä½ çš„ sampling.py é‡Œæœ‰ mnist_dirichlet å°±ç”¨å®ƒï¼Œæ²¡æœ‰çš„è¯ç›´æ¥ç”¨ cifar_dirichlet å¤„ç† MNIST æ ‡ç­¾ä¹Ÿæ˜¯ä¸€æ ·çš„
            try:
                from utils.sampling import mnist_dirichlet
                dict_users = mnist_dirichlet(dataset_train, args.num_users, args.alpha)
            except ImportError:
                dict_users = cifar_dirichlet(dataset_train, args.num_users, args.alpha,args.local_bs)
        else:
            exit('Error: unrecognized partition strategy for MNIST')
            
    elif args.dataset == 'cifar':
        trans_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),     # éšæœºè£å‰ªï¼ˆæ ‡å‡†CIFARå¢å¼ºï¼‰
        transforms.RandomHorizontalFlip(),        # éšæœºæ°´å¹³ç¿»è½¬
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), 
                             (0.5, 0.5, 0.5))
        ])
        trans_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), 
                             (0.5, 0.5, 0.5))
        ])
        dataset_train = datasets.CIFAR10(
        '../data/cifar', 
        train=True, 
        download=True, 
        transform=trans_train
        )
        dataset_test = datasets.CIFAR10(
        '../data/cifar', 
        train=False, 
        download=True, 
        transform=trans_test
        )
        
        if args.partition == 'iid':
            dict_users = cifar_iid(dataset_train, args.num_users)
        elif args.partition == 'shard':
            dict_users = cifar_noniid(dataset_train, args.num_users)
        elif args.partition == 'dirichlet':
            dict_users = cifar_dirichlet(dataset_train, args.num_users, args.alpha,args.local_bs)
        else:
            exit('Error: unrecognized partition strategy for CIFAR')
    else:
        exit('Error: unrecognized dataset')

    # ================= [Build Model] =================
    if args.dataset == 'mnist':
        net_glob = CNNMnist(args=args).to(args.device)
    elif args.dataset == 'cifar':
        net_glob = CNNCifar(args=args).to(args.device)
    
    print(net_glob)
    net_glob.train()
    w_glob = net_glob.state_dict()

   # ================= [æˆ˜å‰å‡†å¤‡ï¼šåªä¿ç•™èµ„æºç®¡ç†å™¨ï¼Œåˆ æ‰æ‰€æœ‰æ•°æ®æ‰“åˆ†] =================
    w_glob = net_glob.state_dict()
    resource_mgr = ResourceManager(args.num_users)
    loss_train = []
    acc_test_history = [] 

    if args.all_clients: 
        print("Aggregation over all clients")
        w_locals = [w_glob for i in range(args.num_users)]

    for iter in range(args.epochs):
        loss_locals = []
        len_locals = [] 
        
        if not args.all_clients:
            w_locals = []
            
        m = max(int(args.frac * args.num_users), 1)

        # ================= [Selection Strategy (Random + Resource Constraints)] =================
        # 1. çº¯éšæœºæ‰“ä¹±æ‰€æœ‰å®¢æˆ·ç«¯
        all_clients = list(range(args.num_users))
        np.random.shuffle(all_clients)
        
        # 2. èµ„æºçº¦æŸä¸‹çš„éšæœºè£…ç®± (Random Knapsack Selection)
        selected_users = []
        current_energy_sum = 0.0
        current_max_time = 0.0
        
        for client_id in all_clients:
            data_len = len(dict_users[client_id])
            t, e = resource_mgr.calculate_cost(client_id, data_len)
            
            potential_energy = current_energy_sum + e
            potential_time = max(current_max_time, t)
            
            # ä¸¥æ ¼æ£€æŸ¥çº¢çº¿ï¼šåªè¦ä¸è¶…æ—¶ã€ä¸è¶…ç”µï¼Œå¹¶ä¸”æ˜¯éšæœºç¢°ä¸Šçš„ï¼Œå°±ç›´æ¥æ‹‰å…¥é˜Ÿä¼
            if potential_energy <= args.max_energy and potential_time <= args.max_time:
                selected_users.append(client_id)
                current_energy_sum = potential_energy
                current_max_time = potential_time
                
            # æ‹›æ»¡ m ä¸ªäººå³å¯åœæ­¢
            if len(selected_users) >= m:
                break
                
        # 3. å­¦æœ¯å…œåº•æœºåˆ¶
        if len(selected_users) == 0:
            print("âš ï¸ è­¦å‘Šï¼šæœ¬è½®ç³»ç»Ÿèµ„æºçº¦æŸè¿‡äºä¸¥è‹›ï¼Œè§¦å‘å…œåº•ï¼å¼ºè¡Œé€‰æ‹© 1 ä¸ªæœ€é«˜æ•ˆèŠ‚ç‚¹ã€‚")
            times = [resource_mgr.calculate_cost(i, len(dict_users[i]))[0] for i in range(args.num_users)]
            fastest_client = np.argmin(times)
            selected_users = [fastest_client]
            current_max_time, current_energy_sum = resource_mgr.calculate_cost(fastest_client, len(dict_users[fastest_client]))
            
        resource_mgr.update_selection(selected_users)
        
        print(f"Round {iter} | éšæœºçº¦æŸè£…ç®± | é€‰ä¸­ {len(selected_users)} äºº | æ—¶å»¶: {current_max_time:.2f}s | è€—èƒ½: {current_energy_sum:.2f}J")
        # ========================================================================================

        # ã€æ ¸å¿ƒä¿®æ­£ã€‘ï¼šçº¯ç²¹çš„æœ¬åœ°è®­ç»ƒï¼Œæ²¡æœ‰ä»»ä½• Sim æ›´æ–°ï¼
        for idx in selected_users:
            local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])
            w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))
            
            if args.all_clients:
                w_locals[idx] = copy.deepcopy(w)
            else:
                w_locals.append(copy.deepcopy(w))
            loss_locals.append(copy.deepcopy(loss))
            
            len_locals.append(len(dict_users[idx]))
            
        # å°†æƒé‡ä¼ é€’ç»™åŠ æƒèšåˆå‡½æ•°
        w_glob = FedAvg(w_locals, len_locals)
        net_glob.load_state_dict(w_glob)


        loss_avg = sum(loss_locals) / len(loss_locals)
        loss_train.append(loss_avg)

        # Evaluation
        net_glob.eval()
        acc_test, loss_test = test_img(net_glob, dataset_test, args)
        acc_test_history.append(acc_test)
        print('Round {:3d}, Average loss {:.3f}, Test Acc {:.2f}%'.format(iter, loss_avg, acc_test))
        net_glob.train()
        args.lr = args.lr * 0.99


    # ================= [ç»˜å›¾ä¸ä¿å­˜ç»“æœ] =================
    import datetime

    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    script_name = os.path.basename(__file__).split('.')[0]
    
    file_id = 'fed_{}_{}_{}_alpha{}_ep{}_time{}_energy{}_{}'.format(
        script_name, 
        args.dataset, 
        args.partition, 
        args.alpha, 
        args.epochs, 
        args.max_time,   # è®°å½•æ—¶é—´çº¢çº¿
        args.max_energy, # è®°å½•èƒ½è€—çº¢çº¿
        timestamp
    )

    # ã€ä¿®å¤5ã€‘å¢åŠ é˜²å´©æºƒç›®å½•æ£€æŸ¥
    save_dir = './save'
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)

    save_path = os.path.join(save_dir, '{}_acc.npy'.format(file_id))
    np.save(save_path, acc_test_history)
    
    print(f"ğŸ‰ å®éªŒç»“æŸï¼æ•°æ®å·²ç»å¯¹å®‰å…¨åœ°ä¿å­˜åˆ°: {save_path}")
