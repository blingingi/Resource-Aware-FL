#!/usr/bin/env python
# -*- coding: utf-8 -*-
# Python version: 3.6

import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import copy
import numpy as np
import torch
import os
from torchvision import datasets, transforms
from torch import nn

# å¼•å…¥é¡¹ç›®ä¾èµ–
from utils.sampling import mnist_iid, mnist_noniid, mnist_dirichlet,cifar_iid, cifar_noniid, cifar_dirichlet
from utils.options import args_parser
from models.Update import LocalUpdate
# ç¡®ä¿ Nets é‡Œçš„ CNNCifar å·²ç»æ˜¯ä½ ä¿®æ”¹è¿‡çš„ 3å±‚å®½ä½“ç‰ˆæœ¬
from models.Nets import MLP, CNNMnist, CNNCifar
from models.Fed import FedAvg
from models.test import test_img
from utils.resource import ResourceManager

# ===================================================================
# è¾…åŠ©å‡½æ•°ï¼šè®¡ç®— KL æ•£åº¦ (Diversity Metric)
# ===================================================================
def calculate_diversity(dataset, dict_users, num_classes):
    diversity_scores = []
    P_uniform = np.ones(num_classes) / num_classes 
    
    print("æ­£åœ¨è®¡ç®—æ‰€æœ‰å®¢æˆ·ç«¯çš„æ•°æ®å¤šæ ·æ€§ (Diversity)...")
    
    for idx in range(len(dict_users)):
        user_indices = dict_users[idx]
        if hasattr(dataset, 'targets'):
            labels = np.array(dataset.targets)[list(user_indices)]
        else:
            labels = dataset.train_labels.numpy()[list(user_indices)]
            
        label_counts = np.zeros(num_classes)
        for label in labels:
            label_counts[label] += 1
        
        P_client = (label_counts + 1e-5) / (sum(label_counts) + 1e-5 * num_classes)
        kl_div = np.sum(P_uniform * np.log(P_uniform / P_client))
        diversity_scores.append(kl_div)
        
    return np.array(diversity_scores)

# ===================================================================
# è¾…åŠ©å‡½æ•°ï¼šè®¡ç®—ç›¸ä¼¼æ€§ (Similarity Metric)
# åªè®¡ç®—æœ€åä¸€å±‚ (fc2) çš„å‚æ•°è·ç¦»ï¼Œé˜²æ­¢è·ç¦»è¿‡å¤§å¯¼è‡´åˆ†æ•°å½’é›¶
# ===================================================================
def calculate_similarity_score(w_global, w_local, k1=10, k2=0.01):
    diff_sum = 0
    target_layer = 'fc2' 
    
    layer_found = False
    for k in w_global.keys():
        if target_layer in k:
            diff_sum += torch.sum(torch.abs(w_global[k] - w_local[k])).item()
            layer_found = True
            
    if not layer_found:
        total_diff = 0
        total_params = 0
        for k in w_global.keys():
            total_diff += torch.sum(torch.abs(w_global[k] - w_local[k])).item()
            total_params += w_global[k].numel()
        diff_sum = total_diff
    
    rho = diff_sum
    sim = k1 * np.exp(-k2 * rho)
    return sim

if __name__ == '__main__':
    args = args_parser()
    args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')

   # ================= [Load Dataset] =================
    if args.dataset == 'mnist':
        trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])
        dataset_train = datasets.MNIST('../data/mnist/', train=True, download=True, transform=trans_mnist)
        dataset_test = datasets.MNIST('../data/mnist/', train=False, download=True, transform=trans_mnist)
        
        # å…¼å®¹ä¸åŒçš„åˆ’åˆ†æ–¹å¼
        if args.partition == 'iid':
            dict_users = mnist_iid(dataset_train, args.num_users)
        elif args.partition == 'shard':
            dict_users = mnist_noniid(dataset_train, args.num_users)
        elif args.partition == 'dirichlet':
            # å¦‚æœä½ çš„ sampling.py é‡Œæœ‰ mnist_dirichlet å°±ç”¨å®ƒï¼Œæ²¡æœ‰çš„è¯ç›´æ¥ç”¨ cifar_dirichlet å¤„ç† MNIST æ ‡ç­¾ä¹Ÿæ˜¯ä¸€æ ·çš„
            try:
                from utils.sampling import mnist_dirichlet
                dict_users = mnist_dirichlet(dataset_train, args.num_users, args.alpha)
            except ImportError:
                dict_users = cifar_dirichlet(dataset_train, args.num_users, args.alpha)
        else:
            exit('Error: unrecognized partition strategy for MNIST')
            
    elif args.dataset == 'cifar':
        trans_cifar = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
        dataset_train = datasets.CIFAR10('../data/cifar', train=True, download=True, transform=trans_cifar)
        dataset_test = datasets.CIFAR10('../data/cifar', train=False, download=True, transform=trans_cifar)
        
        if args.partition == 'iid':
            dict_users = cifar_iid(dataset_train, args.num_users)
        elif args.partition == 'shard':
            dict_users = cifar_noniid(dataset_train, args.num_users)
        elif args.partition == 'dirichlet':
            dict_users = cifar_dirichlet(dataset_train, args.num_users, args.alpha)
        else:
            exit('Error: unrecognized partition strategy for CIFAR')
    else:
        exit('Error: unrecognized dataset')

    # ================= [Build Model] =================
    if args.dataset == 'mnist':
        net_glob = CNNMnist(args=args).to(args.device)
    elif args.dataset == 'cifar':
        net_glob = CNNCifar(args=args).to(args.device)
    
    print(net_glob)
    net_glob.train()
    w_glob = net_glob.state_dict()

    # ================= [ç­–ç•¥ä¿®æ­£] =================
    # 1. Diversity
    div_scores = calculate_diversity(dataset_train, dict_users, args.num_classes)
    div_min, div_max = div_scores.min(), div_scores.max()
    div_norm = (div_scores - div_min) / (div_max - div_min + 1e-8)
    
    # 2. Similarity
    sim_scores = np.ones(args.num_users) * 10.0 # åˆå§‹ç»™ä¸ªé«˜åˆ†
    
    # 3. æƒé‡å‚æ•°
    alpha_1 = 0.2  # Similarity æƒé‡
    alpha_2 = 0.8  # Diversity æƒé‡
    # ============================================
    resource_mgr = ResourceManager(args.num_users)
    loss_train = []
    acc_test_history = [] 

    if args.all_clients: 
        print("Aggregation over all clients")
        w_locals = [w_glob for i in range(args.num_users)]

    for iter in range(args.epochs):
        loss_locals = []
        len_locals = [] # ã€ä¿®å¤1ã€‘åˆå§‹åŒ–å½“å‰è½®æ¬¡çš„æ•°æ®é‡è®°å½•åˆ—è¡¨
        
        if not args.all_clients:
            w_locals = []
            
        m = max(int(args.frac * args.num_users), 1)

      # ================= [Selection Strategy (Random + Resource Constraints)] =================
        # 1. å½»åº•æ”¾å¼ƒæ•°æ®ä»·å€¼æ‰“åˆ†ï¼Œç›´æ¥çº¯éšæœºæ‰“ä¹±æ‰€æœ‰å®¢æˆ·ç«¯
        all_clients = list(range(args.num_users))
        np.random.shuffle(all_clients)
        
        # 2. èµ„æºçº¦æŸä¸‹çš„éšæœºè£…ç®± (Random Knapsack Selection)
        selected_users = []
        current_energy_sum = 0.0
        current_max_time = 0.0
        
        for client_id in all_clients:
            data_len = len(dict_users[client_id])
            t, e = resource_mgr.calculate_cost(client_id, data_len)
            
            potential_energy = current_energy_sum + e
            potential_time = max(current_max_time, t)
            
            # ä¸¥æ ¼æ£€æŸ¥çº¢çº¿ï¼šåªè¦ä¸è¶…æ—¶ã€ä¸è¶…ç”µï¼Œå¹¶ä¸”æ˜¯éšæœºç¢°ä¸Šçš„ï¼Œå°±ç›´æ¥æ‹‰å…¥é˜Ÿä¼
            if potential_energy <= args.max_energy and potential_time <= args.max_time:
                selected_users.append(client_id)
                current_energy_sum = potential_energy
                current_max_time = potential_time
                
            # æ‹›æ»¡ m ä¸ªäººå³å¯åœæ­¢
            if len(selected_users) >= m:
                break
                
        # 3. å­¦æœ¯å…œåº•æœºåˆ¶ï¼šé˜²æ­¢å‚æ•°è®¾å¾—å¤ªä¸¥å¯¼è‡´æœ¬è½®æ— äººå¯ç”¨
        if len(selected_users) == 0:
            print("âš ï¸ è­¦å‘Šï¼šæœ¬è½®ç³»ç»Ÿèµ„æºçº¦æŸè¿‡äºä¸¥è‹›ï¼Œè§¦å‘å…œåº•ï¼å¼ºè¡Œé€‰æ‹© 1 ä¸ªæœ€é«˜æ•ˆèŠ‚ç‚¹ã€‚")
            times = [resource_mgr.calculate_cost(i, len(dict_users[i]))[0] for i in range(args.num_users)]
            fastest_client = np.argmin(times)
            selected_users = [fastest_client]
            current_max_time, current_energy_sum = resource_mgr.calculate_cost(fastest_client, len(dict_users[fastest_client]))
            
        # è®°å½•è¢«é€‰ä¸­çš„èŠ‚ç‚¹
        resource_mgr.update_selection(selected_users)
        
        print(f"Round {iter} | éšæœºçº¦æŸè£…ç®± | é€‰ä¸­ {len(selected_users)} äºº | æ—¶å»¶: {current_max_time:.2f}s | è€—èƒ½: {current_energy_sum:.2f}J")
        # ========================================================
        # ========================================================================

        # ã€ä¿®å¤å®Œæˆã€‘åªä¿ç•™ä¸€ä¸ªå¾ªç¯ï¼Œä¸”ä¸¥æ ¼éå† selected_users
        for idx in selected_users:
            local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])
            w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))
            
            new_sim = calculate_similarity_score(w_glob, w)
            sim_scores[idx] = new_sim
            
            if args.all_clients:
                w_locals[idx] = copy.deepcopy(w)
            else:
                w_locals.append(copy.deepcopy(w))
            loss_locals.append(copy.deepcopy(loss))
            
            # ä¸¥æ ¼è®°å½•è¢«é€‰å®¢æˆ·ç«¯çš„çœŸå®æ ·æœ¬æ•°
            len_locals.append(len(dict_users[idx]))
            
        # å°†æƒé‡ä¼ é€’ç»™åŠ æƒèšåˆå‡½æ•°
        w_glob = FedAvg(w_locals, len_locals)
        net_glob.load_state_dict(w_glob)

        loss_avg = sum(loss_locals) / len(loss_locals)
        loss_train.append(loss_avg)

        # Evaluation
        net_glob.eval()
        acc_test, loss_test = test_img(net_glob, dataset_test, args)
        acc_test_history.append(acc_test)
        print('Round {:3d}, Average loss {:.3f}, Test Acc {:.2f}%'.format(iter, loss_avg, acc_test))
        net_glob.train()
        args.lr = args.lr * 0.99


    # ================= [ç»˜å›¾ä¸ä¿å­˜ç»“æœ] =================
    import datetime

    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    script_name = os.path.basename(__file__).split('.')[0]
    
    file_id = 'fed_{}_{}_{}_alpha{}_ep{}_time{}_energy{}_{}'.format(
        script_name, 
        args.dataset, 
        args.partition, 
        args.alpha, 
        args.epochs, 
        args.max_time,   # è®°å½•æ—¶é—´çº¢çº¿
        args.max_energy, # è®°å½•èƒ½è€—çº¢çº¿
        timestamp
    )

    # ã€ä¿®å¤5ã€‘å¢åŠ é˜²å´©æºƒç›®å½•æ£€æŸ¥
    save_dir = './save'
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)

    save_path = os.path.join(save_dir, '{}_acc.npy'.format(file_id))
    np.save(save_path, acc_test_history)
    
    print(f"ğŸ‰ å®éªŒç»“æŸï¼æ•°æ®å·²ç»å¯¹å®‰å…¨åœ°ä¿å­˜åˆ°: {save_path}")
